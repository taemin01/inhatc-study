##### 다음 중 케라스에서 모델을 생성하고 레이어를 추가하는 코드로 올바른 것은? 
정답 : 3번
```python 
#1.
- `model = keras.Sequential()`  
    `model.add(Dense(units=2, activation='relu'))`
#2.	
- `model = keras.Sequential`  
    `model.add(Dense(2, activation='relu'))`
#3.    
- `model = keras.Sequential()`  
    `model.add(Dense(units=2, activation='relu', input_shape=(2,)))`
#4.   
- `model = keras.Sequential()`  
    `model.add(Dense(activation='relu', units=2, input_shape=(2,)))`
    # 복붙이라 ` ` -> 백틱 무시하고 코드만 보면 됨
```
2번이라 했다가 틀림  
Sequential 뒤 괄호가 없었고 모델을 정의하고 나서 Dense 레이어를 추가할 때 올바른 형태는  
3번의 코드이다. 첫 번째 레이어에는 input_shape를 지정해줘야 한다.  
4번 문법상 가능은 하지만 units -> activation -> input_shape 순서로 많이 작성한다.  

##### 하이퍼 파라미터
하이퍼 파라미터는 학습 전에 사람이 설정하는 변수이다.   
학습률, 은닉층 개수, 배치 사이즈 등이 하이퍼 파라미터이다.  
가중치의 초기값은 보통 난수로 자동 설정되거나 특정 초기화 방법을 사용하며, 사람이 직접  
튜닝하는 변수는 아니다. 그래서 하이퍼 파라미터가 아니다.  

##### 활성화 함수 역할 설명
활성화 함수는 인공신경망에서 각 뉴런의 출력값을 결정하는 함수이다.  
입력 신호, 가중치 곱을 모두 더한 값에 비선형성을 부여해, 신경망이  
복잡한 패턴을 학습할 수 있게 만든다. Relu, sigmoid, tanh 등이 있다.  
이 함수가 없다면 신경망은 단순한 선형 모델과 같아져 복잡한 문제를 풀 수 없다.

#### units, input_shape
input_shape는 입력 데이터의 형태(차원)를 나타낸다.  
여기서 (4,)는 입력 데이터가 1차원 벡터이고, 크기가 4라는 뜻이다.  
그리고 keras에서 입력 형태가 (28, 28, 1)이면 batch_size가 배치 크기로 들어가  
한 번에 배치 사이즈 개의 3차원 상자가 하나 더 모여 4차원이 된다고 생각하면 된다. 
쉽게 말해 높이 * 너비 * 채널 수의 이미지가 1개 담겨있는 상자가 있다  
배치 크기 64는 그런 상자 64개를 한 묶음으로 묶은 상자이다.  
64개의 이미지를 묶어놓은 3차원 상자가 하나 더 모여 4차원이 되는 것이다.  
64개의 이미지가 동시에 신경망으로 들어간다는 뜻.  

##### Ont-Hot Encoding
원 핫 인코딩은 범주형 데이터의 각 클래스마다 고유한 인덱스 위치에만 1을 부여하고  
나머지는 0으로 만드는 인코딩 방식이다. 즉 0과 1의 조합으로 표현하는 기법이라 할 수 있다.  
각 클래스마다 고유한 정수값을 부여하는 것은 Lable Encoding 방식이다.

##### 이미지 데이터 전처리
색상 공간을 CMYK로 변환하는 건 일반적으로 전처리에서 하지 않는다.  
대부분 모델은 RGB를 그대로 쓰거나 필요하면 흑백으로 변환하는 경우는 있다.  

##### vaild, same
vaild는 패딩을 하지 않는다는 뜻이며, 커널이 이미지 안에서만 움직인다.  
가장자리 픽셀에 필터를 완전히 적용할 수 없기에 줄어드는 것이다.  
계산은
출력 너비 = 입력 너비 - 출력 너비 + 스트라이드  
출력 높이 = 입력 높이 - 출력 높이 + 스트라이드  
이 형태의 계산이 더 쉬운 편이고, 입력은 전 레이어에서의 형태이자 첫 input_shape이다.  
출력은 해당 레이어에 kernel_size이다 숫자가 같으면 굳이 나눠서 계산하지 않아도 된다.  

same 패딩은 입출력 크기를 같게 유지하기 위해서 입력 주변에 0으로 패딩(빈 칸)을 추가한다.  
커널이 이미지 가장자리까지 완전히 적용되어 출력 크기가 입력과 같다.
출력 크기가 입력 크기와 동일하다.  
입력 크기 : 28 28  
출력 크기 : 3 3  
스트라이드 : 1 이렇게 있어도 가장자리까지 적용되어 크기가 동일해진다.  

##### 활성화 함수
Relu는 입력이 0 이하일 때 출력을 0으로 만든다.  
CNN에서 가장 많이 사용하는 활성화 함수이다. 입력이 0보다 크면 그대로 출력하고 이하면  
0으로 출력해서 비선형성을 부여한다. 계산이 간단하고 학습 속도가 빠르며, 깊은 신경망에서 성능이 좋다.  

시그모이드는 출력값이 항상 0과 1사이에 있다. 주로 이진 분류 문제의 출력층에서 사용된다.  

소프트맥스 함수는 다중 클래스 분류에서 각 클래스의 확률을 출력한다.    
tanh는 시그모이드와 비슷하지만 출력값 범위가 -1부터 1까지이다.  
시그모이드보다 성능이 좋지만 렐루보다는 덜 쓰인다.
##### 배치 정규화
배치 정규화는 딥러닝 모델 학습 시 각 층에 입력되는 데이터 분포가 변하는 현상을  
줄이기 위한 기법이다. 예시 현상은 내부 공변량 변화, Internal Covariate Shift가 있다.  

##### 과적합 방지 기법 3가지
밑에 정리해놓은 Dropout이 한 가지 방법이다.  
데이터 증대라는 기법이 있다, 회전, 확대, 축소, 좌우 반전 등으로 변형하여 데이터셋을 인위적으로 늘리며  
다양한 패턴을 학습하게 하며 과적합을 방지한다.  

또 다른 방법은 Ealry Stopping으로 일정 epoch 이상 검증 loss 값이 개선되지 않으면   
가장 성능이 좋았던 시점의 모델 가중치를 저장한다.  
정규화 : 입력값 범위를 일정하게 맞추어 빠르고 안정적으로 학습되게 하는 것  
과적합 보다는 학습 안정화를 위함이지만 문맥상 같이 묶일 때도 있다.

##### Dropout의 역할 주관식
딥러닝 모델이 학습할 때 일부 뉴런(노드)을 랜덤하게 비활성화 하는 기법이다.  
일정 비율의 노드를 임의로 제거해서 학습하고 다음 학습 땐 또 다른 노드를 랜덤하게 비활성화 한다.  
사용 이유는 역시 과적합을 방지하기 위해서 사용된다. 막는 동작 방식은 비활성화를 통해  
특정 노드나 경로에만 의존하는 것을 막아준다.  
비활성화 라는 것은 삭제하는 것이 아닌 0으로 만들어서 참여하지 못 하게 만든 것이다.  

##### self-attention
은 입력 시퀀스 내의 각 단어가 다른 단어들과 얼마나 관련 있는지 가중치를 계산하는 메커니즘이다.  
즉, 문장 내 단어들이 서로 주의를 주고받으며 의미를 파악하는 방법이다.

##### 파라미터 계산 방법 - 레이어의 Param 계산과 다름
입력값 개수 : 3, 은닉층 유닛 수 : 4, 출력층 유닛 수 : 2  
각 유닛 당 바이어스 값 1개씩이라고 가정했을 때  
입력층 -> 은닉층 계산 과정  
가중치 : 3 * 4 = 12 / 바이어스 : 4 1개씩이라고 가정했기에 이렇게 나옴  
총 : 12 + 4 = 16  

은닉층 -> 출력층  
가중치 : 4 * 2 = 8  
바이어스 : 2 이것도 동일하게 유닛마다 1  가정하여 출력 유닛이 2개 이기에 2가 나옴  
총 : 8 + 2 = 10  

그래서 전체 파라미터 개수는 16 + 10으로 26개가 된다.  