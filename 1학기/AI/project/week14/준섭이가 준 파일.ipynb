{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 과제 제출용"
      ],
      "metadata": {
        "id": "Yux8F-3HMTzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import *\n",
        "\n",
        "nltk.download('punkt') # 구두점 제거\n",
        "\n",
        "def homework_nltk(text: str):\n",
        "  tokens = word_tokenize(text)\n",
        "  return [word for word in tokens if word.isalpha()]\n",
        "\n",
        "# 과제1\n",
        "print(homework_nltk('테스트 테스트1 입니다.'))\n",
        "\n",
        "# 과제2\n",
        "print(text_to_word_sequence(\"This is a dog.\"))"
      ],
      "metadata": {
        "id": "kjHZVaMLMQiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 공부용"
      ],
      "metadata": {
        "id": "AKdfYfwPMRZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNBsO4owGiKs",
        "outputId": "fd4ee37a-382e-475a-98ad-c99780a9888f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt') # 구두점 제거"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(\"Hello World!, This is a dog11...!\")\n",
        "\n",
        "print(tokens) # 단어단위로 리스트 반환\n",
        "\n",
        "# 문자나 숫자인 경우에만 단어를 리스트에 추가한다.\n",
        "words = [word for word in tokens if word.isalpha()] # 숫자가 들어있는 단어도 X\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7Ye-86uHerJ",
        "outputId": "c51d153b-0697-49a2-deba-0930ca05a1d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'World', '!', ',', 'This', 'is', 'a', 'dog11', '...', '!']\n",
            "['Hello', 'World', 'This', 'is', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__HCiks8IEqA",
        "outputId": "d010973f-7ea3-4838-d50f-fc8ad2d87fee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어단위\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"This is a dog.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVT0n8IZIzJJ",
        "outputId": "6de17acc-0f1d-4f50-9859-84c0c0f4a804"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장단위\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"This is a house. This is a dog.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EclYR77SJBAx",
        "outputId": "21baad3f-260b-4b24-efdb-ccd69b90dc9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a house.', 'This is a dog.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import *\n",
        "print(text_to_word_sequence(\"This is a dog.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXTgWVzTLexG",
        "outputId": "bfe121da-278b-404c-9443-b028a30cf5b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'dog']\n"
          ]
        }
      ]
    }
  ]
}