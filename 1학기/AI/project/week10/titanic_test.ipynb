{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w31G0dhZuqbx",
        "outputId": "972784ad-173a-4646-8bc7-2c68b1ae4a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441us/step - accuracy: 0.6557 - loss: 0.5950\n",
            "Epoch 2/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - accuracy: 0.7697 - loss: 0.5538\n",
            "Epoch 3/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - accuracy: 0.7947 - loss: 0.5171\n",
            "Epoch 4/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - accuracy: 0.7889 - loss: 0.5055\n",
            "Epoch 5/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.7976 - loss: 0.4863\n",
            "Epoch 6/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.7680 - loss: 0.4899\n",
            "Epoch 7/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - accuracy: 0.7577 - loss: 0.5010\n",
            "Epoch 8/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - accuracy: 0.7858 - loss: 0.4759\n",
            "Epoch 9/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - accuracy: 0.8042 - loss: 0.4544\n",
            "Epoch 10/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.7782 - loss: 0.4959\n",
            "Epoch 11/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.7970 - loss: 0.4573\n",
            "Epoch 12/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - accuracy: 0.7851 - loss: 0.4471\n",
            "Epoch 13/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - accuracy: 0.7655 - loss: 0.4822\n",
            "Epoch 14/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.7873 - loss: 0.4404\n",
            "Epoch 15/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - accuracy: 0.7808 - loss: 0.4615\n",
            "Epoch 16/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.7834 - loss: 0.4587\n",
            "Epoch 17/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.7896 - loss: 0.4693\n",
            "Epoch 18/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.7916 - loss: 0.4448\n",
            "Epoch 19/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.8070 - loss: 0.4364\n",
            "Epoch 20/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.7716 - loss: 0.4788\n",
            "Epoch 21/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - accuracy: 0.7812 - loss: 0.4827\n",
            "Epoch 22/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.7992 - loss: 0.4377\n",
            "Epoch 23/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - accuracy: 0.7895 - loss: 0.4530\n",
            "Epoch 24/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.7844 - loss: 0.4589\n",
            "Epoch 25/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - accuracy: 0.7829 - loss: 0.4497\n",
            "Epoch 26/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step - accuracy: 0.8061 - loss: 0.4189\n",
            "Epoch 27/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.8074 - loss: 0.4469\n",
            "Epoch 28/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.7611 - loss: 0.4930\n",
            "Epoch 29/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - accuracy: 0.8322 - loss: 0.4150\n",
            "Epoch 30/30\n",
            "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.7856 - loss: 0.4445\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1571e7a50>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# 데이터 세트를 읽어들인다. \n",
        "train = pd.read_csv(\"train.csv\", sep=',')\n",
        "test = pd.read_csv(\"test.csv\", sep=',')\n",
        "\n",
        "# 필요없는 컬럼을 삭제한다. \n",
        "train.drop(['SibSp', 'Parch', 'Ticket', 'Embarked', 'Name',\\\n",
        "        'Cabin', 'PassengerId', 'Fare', 'Age'], inplace=True, axis=1)\n",
        "test.drop(['SibSp', 'Parch', 'Ticket', 'Embarked', 'Name',\\\n",
        "        'Cabin', 'PassengerId', 'Fare', 'Age'], inplace=True, axis=1)\n",
        "\n",
        "# 결손치가 있는 데이터 행은 삭제한다. \n",
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)\n",
        "\n",
        "# 기호를 수치로 변환한다. \n",
        "for ix in train.index:\n",
        "    if train.loc[ix, 'Sex']==\"male\":\n",
        "       train.loc[ix, 'Sex']=1 \n",
        "    else:\n",
        "       train.loc[ix, 'Sex']=0 \n",
        "for ix in test.index:\n",
        "    if test.loc[ix, 'Sex']==\"male\":\n",
        "       test.loc[ix, 'Sex']=1 \n",
        "    else:\n",
        "       test.loc[ix, 'Sex']=0 \n",
        "\n",
        "\n",
        "# 2차원 배열을 1차원 배열로 평탄화한다. \n",
        "target = np.ravel(train.Survived) \n",
        "\n",
        "\n",
        "# 생존여부를 학습 데이터에서 삭제한다. \n",
        "train.drop(['Survived'], inplace=True, axis=1)\n",
        "train = train.astype(float)     # 최근 소스에서는 float형태로 형변환하여야 \n",
        "test = test.astype(float)     # 최근 소스에서는 float형태로 형변환하여야 \n",
        "\n",
        "\n",
        "\n",
        "# 케라스 모델을 생성한다. \n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(2,)))\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 케라스 모델을 컴파일한다. \n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "# 케라스 모델을 학습시킨다. \n",
        "model.fit(train, target, epochs=30, batch_size=1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv0UwpHIyfo7",
        "outputId": "e8ac5b75-3169-4f8d-a20b-3606511ec203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 0.] [0.84050065]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622247]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 0.] [0.84050065]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622247]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622247]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 0.] [0.84050065]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.8405006]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 0.] [0.84050065]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 0.] [0.84050065]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 0.] [0.84050065]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 0.] [0.42622247]\n",
            "[3. 0.] [0.42622247]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 0.] [0.84050065]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[2. 1.] [0.20062831]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 0.] [0.9667859]\n",
            "[1. 1.] [0.35509673]\n",
            "[1. 1.] [0.35509673]\n",
            "[2. 1.] [0.20062831]\n",
            "[2. 1.] [0.20062831]\n",
            "[1. 1.] [0.35509673]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 0.] [0.42622244]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 0.] [0.42622244]\n",
            "[3. 1.] [0.11963955]\n",
            "[1. 0.] [0.9667859]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n",
            "[3. 1.] [0.11963955]\n"
          ]
        }
      ],
      "source": [
        "idx=test.shape[0]\n",
        "pred=model.predict(test)\n",
        "test_np=test.to_numpy()\n",
        "for i in range(idx):\n",
        "   print(test_np[i,:],pred[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exuUtRpKwc-z",
        "outputId": "3101d1fb-9fe2-4edd-e0c9-968bde62b11e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   PassengerId  Pclass                                          Name     Sex  \\\n",
            "0          892       3                              Kelly, Mr. James    male   \n",
            "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
            "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
            "3          895       3                              Wirz, Mr. Albert    male   \n",
            "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
            "\n",
            "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
            "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
            "1  47.0      1      0   363272   7.0000   NaN        S  \n",
            "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
            "3  27.0      0      0   315154   8.6625   NaN        S  \n",
            "4  22.0      1      1  3101298  12.2875   NaN        S  \n"
          ]
        }
      ],
      "source": [
        "print(test.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled43.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
